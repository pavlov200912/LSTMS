# -*- coding: utf-8 -*-
"""dedlstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FsslGguRNWSdDUXuAomResHYkHPmD1ZI
"""

import re
import nltk
from random import randint
from nltk.tokenize import word_tokenize
from rupo.api import Engine
import numpy as np
from pickle import dump
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.models import Sequential
from keras.callbacks import LambdaCallback
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from random import randint
from pickle import load
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences

MIN_WORDS_FREQ = 1
SEQ_SIZE = 10
ENGINE = None


class MyTokenizer:
    def __init__(self):
        self.id_to_word = {}
        self.word_to_id = {}
        self.cleaner = clean_text
        self.tokenizer = word_tokenize

    def set_cleaner(self, cleaner):
        self.cleaner = cleaner

    def set_tokenizer(self, token_f):
        self.tokenizer = token_f

    def fit(self, text_to_fit):
        text_to_fit = self.cleaner(text_to_fit)
        tokens = set(word_tokenize(text_to_fit))
        self.id_to_word = {i: v for i, v in enumerate(tokens)}
        self.word_to_id = {v: i for i, v in enumerate(tokens)}

    def text_to_seq(self, lines):
        res = []
        for line in lines:
            line_res = []
            tokens = self.tokenizer(line)
            for word in tokens:
                line_res.append(self.word_to_id[word])
            res.append(line_res)
        return res


def clean_text(text):
    text = text.lower()
    return re.sub(r'[^А-Яа-я\s\n]', '', text)


def build_sequences(text, words):
    length = SEQ_SIZE + 1
    next_words = []
    sequences = []
    for i in range(length, len(text)):
        seq = text[i - length:i]
        next_words.append(seq[-1])
        sequences.append(' '.join(seq))
    print('Total Sequences: %d' % len(sequences))
    return sequences, next_words


def save_seq(seqs, nexts):
    with open('pushkinSEQ.txt', 'w') as f:
        for i in range(len(seqs)):
            str = ' '.join(seqs[i])
            str += f": {next_words[i]} \n"
            f.write(str)


nltk.download('punkt')
with open('pushkin.txt', encoding='utf-8') as f:
    text = f.read()

# TODO: DELETE THIS
#text = text[:10000]

text = clean_text(text)
tokens = word_tokenize(text)  # ~100k words

print('Loading engine...')

# Load engine
ENGINE = Engine(language='ru')

# Takes long time
ENGINE.load('~/AutoPoetry/stress_ru.h5', '/home/sp/AutoPoetry/zaliznyak.txt')

print('Engine loaded!')

words = set(tokens)
len(words)

"""
word_freq = {}
for _word in text:
    word_freq[_word] = word_freq.get(_word, 0) + 1

ignored_words = set()
for k, v in word_freq.items():
    if word_freq[k] < MIN_WORDS_FREQ:
        ignored_words.add(k)


print('Unique words before ignoring:', len(words))
print('Ignoring words with frequency <', MIN_WORDS_FREQ)
words = sorted(set(words) - ignored_words)
print('Unique words after ignoring:', len(words))
"""

(lines, next_words) = build_sequences(tokens, words)


tokenizer = MyTokenizer()
tokenizer.fit(text)
sequences = tokenizer.text_to_seq(lines) # That's important, that it's lines, not text
# vocabulary size
vocab_size = len(tokenizer.word_to_id)

# separate into input and output
sequences = np.array(sequences)
X, y = sequences[:, :-1], sequences[:, -1]
y = to_categorical(y, num_classes=vocab_size)
seq_length = X.shape[1]
print(X.shape)
print(y.shape)



def generate_probas(preds, temperature):
    # TODO: rewrite this
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    #probas = np.random.multinomial(1, preds, 1)  # Why multinomial?
    return preds


def get_word(prediction, mask, temperature=1.0):
    p = generate_probas(prediction, temperature)
    return tokenizer.id_to_word[np.argmax(np.multiply(p, mask))]


def generate_seq(model, seq_length, seed_text, n_words):
    result = list()
    in_text = seed_text
    # generate a fixed number of words

    mask = generate_stress_mask(words, 1)
    for _ in range(n_words):
        # encode the text as integer
        encoded = tokenizer.text_to_seq([in_text])[0]
        # truncate sequences to a fixed length
        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')
        # predict probabilities for each word
        prediction = model.predict(encoded)[0]
        print("I am asking for a word")
        predicted_word = get_word(prediction, mask)
        result.append(predicted_word)
        in_text += ' ' + predicted_word
    return ' '.join(result)


def get_stress(word):
    return ENGINE.get_stresses(word)[0]


def check_syllables(word, syllable_number):
    vowels_pos = get_word_vowels(word)
    if not vowels_pos:
        return False
    if syllable_number < 1 or syllable_number > len(vowels_pos):
        return False
    if get_stress(word) == vowels_pos[syllable_number - 1]:
        return True


def generate_stress_mask(words, syllable_number):
    # TODO: add oneliner
    res = np.zeros(len(words))
    for i, word in enumerate(words):
        # TODO: This is a pure crutch. In case res[len(words)] why is this happening? idk
        if i == len(words):
            break
        if check_syllables(word, syllable_number):
            res[tokenizer.word_to_id[word]] = 1
    return res


def print_words_on_mask(mask, words):
    for i, val in enumerate(mask):
        if val:
            print(tokenizer.id_to_word[i])


def get_word_vowels(word):
    vowels = ['и', 'а', 'у', 'е', 'о', 'ы', 'ю', 'я', 'э', 'ё']
    vowels_pos = []
    for i, c in enumerate(word):
        if c in vowels:
            vowels_pos.append(i)
    return vowels_pos


def generate_syllables_count_mask(words, syllables_count):
    # TODO: add oneliner
    res = np.zeros(len(words))
    for i, word in enumerate(words):
        if len(get_word_vowels(word)) == syllables_count:
            res[tokenizer.word_to_id[word]] = 1
    return res


# for word in words:
#    print(tokenizer.word_index[word])

# define model
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())


def on_epoch_end(epoch, logs):
    # select a seed text
    seed_text = lines[randint(0, len(lines))]
    print(seed_text + '\n')

    # generate new text
    generated = generate_seq(model, seq_length, seed_text, 10)
    print(f"On seed: {seed_text}\n Generated: {generated}\n")


# compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print_callback = LambdaCallback(on_epoch_end=on_epoch_end)

model.fit(X, y, batch_size=128, epochs=100, callbacks=[print_callback])

model.save('model.h5')
# save the tokenizer
dump(tokenizer, open('tokenizer.pkl', 'wb'))
