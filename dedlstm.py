# -*- coding: utf-8 -*-
"""dedlstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FsslGguRNWSdDUXuAomResHYkHPmD1ZI
"""

import re
import nltk
from random import randint
from nltk.tokenize import word_tokenize
from rupo.api import Engine
import numpy as np
from pickle import dump
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.models import Sequential
from keras.callbacks import LambdaCallback
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from random import randint
from pickle import load
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences

MIN_WORDS_FREQ = 1
SEQ_SIZE = 10
ENGINE = None


def clean_text(text):
    text = text.lower()
    return re.sub(r'[^А-Яа-я\s\n]', '', text)


def build_sequences(text, words):
    length = SEQ_SIZE + 1
    next_words = []
    sequences = []
    for i in range(length, len(text)):
        seq = text[i - length:i]
        next_words.append(seq[-1])
        sequences.append(' '.join(seq))
    print('Total Sequences: %d' % len(sequences))
    return sequences, next_words


def save_seq(seqs, nexts):
    with open('pushkinSEQ.txt', 'w') as f:
        for i in range(len(seqs)):
            str = ' '.join(seqs[i])
            str += f": {next_words[i]} \n"
            f.write(str)


nltk.download('punkt')
with open('pushkin.txt', encoding='utf-8') as f:
    text = f.read()

# TODO: DELETE THIS
text = text[:10000]

text = clean_text(text)
text = word_tokenize(text)  # ~100k words

print('Loading engine...')

# Load engine
ENGINE = Engine(language='ru')

# Takes long time
ENGINE.load('~/AutoPoetry/stress_ru.h5', '/home/sp/AutoPoetry/zaliznyak.txt')

print('Engine loaded!')

words = set(text)
len(words)

"""
word_freq = {}
for _word in text:
    word_freq[_word] = word_freq.get(_word, 0) + 1

ignored_words = set()
for k, v in word_freq.items():
    if word_freq[k] < MIN_WORDS_FREQ:
        ignored_words.add(k)


print('Unique words before ignoring:', len(words))
print('Ignoring words with frequency <', MIN_WORDS_FREQ)
words = sorted(set(words) - ignored_words)
print('Unique words after ignoring:', len(words))
"""

(lines, next_words) = build_sequences(text, words)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(lines)
sequences = tokenizer.texts_to_sequences(lines)
# vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# separate into input and output
sequences = np.array(sequences)
X, y = sequences[:, :-1], sequences[:, -1]
y = to_categorical(y, num_classes=vocab_size)
seq_length = X.shape[1]

rev = dict(map(reversed, tokenizer.word_index.items()))


def generate_probas(preds, temperature):
    # TODO: rewrite this
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)  # Why multinomial?
    return probas


def get_word(prediction, temperature=1.0):
    p = generate_probas(prediction, temperature)
    mask = generate_stress_mask(words, 1)
    return rev[np.argmax(np.dot(p, mask))]


def generate_seq(model, tokenizer, seq_length, seed_text, n_words):
    result = list()
    in_text = seed_text
    # generate a fixed number of words
    for _ in range(n_words):
        # encode the text as integer
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        # truncate sequences to a fixed length
        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')
        # predict probabilities for each word
        prediction = model.predict(encoded)[0]
        predicted_word = get_word(prediction)
        result.append(predicted_word)
        in_text += ' ' + predicted_word
    return ' '.join(result)


def get_stress(word):
    return ENGINE.get_stresses(word)[0]


def check_syllables(word, syllable_number):
    vowels_pos = get_word_vowels(word)
    if not vowels_pos:
        return False
    if syllable_number < 1 or syllable_number > len(vowels_pos):
        return False
    if get_stress(word) == vowels_pos[syllable_number - 1]:
        return True


def generate_stress_mask(words, syllable_number):
    # TODO: add oneliner
    res = np.zeros(len(words))
    for i, word in enumerate(words):
        if check_syllables(word, syllable_number):
            res[tokenizer.word_index[word]] = 1
    return res


def get_word_vowels(word):
    vowels = ['и', 'а', 'у', 'е', 'о', 'ы', 'ю', 'я', 'э', 'ё']
    vowels_pos = []
    for i, c in enumerate(word):
        if c in vowels:
            vowels_pos.append(i)
    return vowels_pos


def generate_syllables_count_mask(words, syllables_count):
    # TODO: add oneliner
    res = np.zeros(len(words))
    for i, word in enumerate(words):
        if len(get_word_vowels(word)) == syllables_count:
            res[tokenizer.word_index[word]] = 1
    return res

#for word in words:
#    print(tokenizer.word_index[word])


# define model
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())


def on_epoch_end(epoch, logs):
    # select a seed text
    seed_text = lines[randint(0, len(lines))]
    print(seed_text + '\n')

    # generate new text
    generated = generate_seq(model, tokenizer, seq_length, seed_text, 10)
    print(f"On seed: {seed_text}\n Generated: {generated}\n")


# compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print_callback = LambdaCallback(on_epoch_end=on_epoch_end)

model.fit(X, y, batch_size=128, epochs=10, callbacks=[print_callback])

model.save('model.h5')
# save the tokenizer
dump(tokenizer, open('tokenizer.pkl', 'wb'))
